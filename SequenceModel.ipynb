{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import layers, optimizers, models\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import csv\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTdata = []\n",
    "directoryDT = './sequenceData/processedDataDonaldTrump.csv'\n",
    "#DTdata_pd = pd.read_csv(directoryDT, delimiter='|')\n",
    "with open(directoryDT, 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in spamreader:\n",
    "        if row !=[]:\n",
    "            r=[]\n",
    "            for col in row:\n",
    "                r.append(float(col))      \n",
    "            DTdata.append(np.array(r))\n",
    "    csvfile.closed\n",
    "\n",
    "\n",
    "NotDTdata = []    \n",
    "directoryNotDT = './sequenceData/NOTrealDonaldTrump_tweetsSequence.csv'\n",
    "#NotDTdata_pd = pd.read_csv(directoryDT, delimiter='|')\n",
    "with open(directoryNotDT, 'r') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter='|')\n",
    "    for row in spamreader:\n",
    "        if row !=[]:\n",
    "            r=[]\n",
    "            for col in row:\n",
    "                r.append(float(col))\n",
    "            NotDTdata.append(np.array(r))   \n",
    "    csvfile.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data to np array\n",
    "DTdata_np = np.array(DTdata)\n",
    "NotDTdata_np = np.array(NotDTdata)\n",
    "\n",
    "#get max length\n",
    "maxLengthDT = len(max(DTdata_np, key=len))\n",
    "maxLengthNotDT = len(max(NotDTdata_np, key=len))\n",
    "maxLength = max([maxLengthDT, maxLengthNotDT])\n",
    "\n",
    "#make zeros matrixes\n",
    "x_data_DT = np.zeros((len(DTdata_np), maxLength))\n",
    "x_data_NotDT = np.zeros((len(NotDTdata_np), maxLength))\n",
    "x_data_DT_oneHot = []\n",
    "x_data_NotDT_oneHot = []\n",
    "\n",
    "#fill the rows with the rows from the data\n",
    "for i in range(len(DTdata_np)):   \n",
    "    newOneHot = []\n",
    "    x_data_DT[i, :len(DTdata_np[i])] = DTdata_np[i]\n",
    "    for e in x_data_DT[i]:\n",
    "        newOneHot.append(to_categorical(e, num_classes=47))\n",
    "    x_data_DT_oneHot.append(np.array(newOneHot))\n",
    "    \n",
    "for i in range(len(NotDTdata_np)):   \n",
    "    newOneHot = []\n",
    "    x_data_NotDT[i, :len(NotDTdata_np[i])] = NotDTdata_np[i]\n",
    "    for e in x_data_NotDT[i]:\n",
    "        newOneHot.append(to_categorical(e, num_classes=47))\n",
    "    x_data_NotDT_oneHot.append(np.array(newOneHot))\n",
    "\n",
    "x_data_DT_oneHot = np.array(x_data_DT_oneHot)\n",
    "x_data_NotDT_oneHot = np.array(x_data_NotDT_oneHot)\n",
    "    \n",
    "\n",
    "#augmentation 100% more data... party, create random sentences which have a bad language\n",
    "x_data_DT_aug = np.zeros((len(DTdata_np), maxLength))\n",
    "x_data_NotDT_aug = np.zeros((len(NotDTdata_np), maxLength))\n",
    "x_data_DT_oneHot_aug = []\n",
    "x_data_NotDT_oneHot_aug = []\n",
    "\n",
    "for i in range(len(DTdata_np)): \n",
    "    newOneHot = []\n",
    "    row = np.array(DTdata_np[i], copy=True)  \n",
    "    np.random.shuffle(row)  \n",
    "    x_data_DT_aug[i, :len(row)] = row\n",
    "    for e in x_data_DT_aug[i]:\n",
    "        newOneHot.append(to_categorical(e, num_classes=47))\n",
    "    x_data_DT_oneHot_aug.append(np.array(newOneHot))\n",
    "    \n",
    "for i in range(len(NotDTdata_np)): \n",
    "    newOneHot = []\n",
    "    row = np.array(NotDTdata_np[i], copy=True)  \n",
    "    np.random.shuffle(row) \n",
    "    x_data_NotDT_aug[i, :len(row)] = row\n",
    "    for e in x_data_NotDT_aug[i]:\n",
    "        newOneHot.append(to_categorical(e, num_classes=47))\n",
    "    x_data_NotDT_oneHot_aug.append(np.array(newOneHot))\n",
    "\n",
    "x_data_DT_oneHot_aug = np.array(x_data_DT_oneHot_aug)\n",
    "x_data_NotDT_oneHot_aug = np.array(x_data_NotDT_oneHot_aug)\n",
    "\n",
    "#make dt tweets 1 and not dt tweets 0 label 2 for random data\n",
    "labelsDT = np.zeros(  (len(x_data_DT),1)  )\n",
    "\n",
    "labelsNotDT = np.zeros((len(x_data_NotDT),1))\n",
    "labelsNotDT[:]=1\n",
    "\n",
    "labelsRandomData1 = np.zeros( (len(DTdata_np),1)  )\n",
    "labelsRandomData1[:]=2\n",
    "\n",
    "labelsRandomData2 = np.zeros( (len(NotDTdata_np),1)  )\n",
    "labelsRandomData2[:]=2\n",
    "\n",
    "#glue the lables and data for random not dt and random dt together\n",
    "labels_RD = np.concatenate([labelsRandomData1,labelsRandomData2])\n",
    "data_RD = np.concatenate([x_data_DT_oneHot_aug, x_data_NotDT_oneHot_aug])\n",
    "\n",
    "#make a validation set for all labels\n",
    "x_train_DT, x_validation_DT, y_train_DT, y_validation_DT = train_test_split(\n",
    "    x_data_DT_oneHot, labelsDT, test_size=0.1, random_state=2)\n",
    "\n",
    "x_train_NotDT, x_validation_NotDT, y_train_NotDT, y_validation_NotDT = train_test_split(\n",
    "    x_data_NotDT_oneHot, labelsNotDT, test_size=0.1, random_state=2)\n",
    "\n",
    "x_train_RD, x_validation_RD, y_train_RD, y_validation_RD = train_test_split(\n",
    "    data_RD, labels_RD, test_size=0.1, random_state=2)\n",
    "\n",
    "x_train_RD1, x_validation_RD1, y_train_RD1, y_validation_RD1 = train_test_split(\n",
    "    x_data_DT_oneHot_aug, labelsRandomData1, test_size=0.1, random_state=2)\n",
    "\n",
    "x_train_RD2, x_validation_RD2, y_train_RD2, y_validation_RD2 = train_test_split(\n",
    "    x_data_NotDT_oneHot_aug, labelsRandomData2, test_size=0.1, random_state=2)\n",
    "\n",
    "\n",
    "# #add labels to the data\n",
    "# data_DT = np.concatenate([y_train_DT, x_train_DT], axis=1)\n",
    "# data_NotDT = np.concatenate([y_train_NotDT, x_train_NotDT], axis=1)\n",
    "# data_RandomData = np.concatenate([y_train_RD, x_train_RD], axis=1)\n",
    "# data_RandomData1 = np.concatenate([y_train_RD1, x_train_RD1], axis=1)\n",
    "# data_RandomData2 = np.concatenate([y_train_RD2, x_train_RD2], axis=1)\n",
    "\n",
    "\n",
    "# #sample data\n",
    "# #data = np.concatenate([data_DT, data_NotDT,data_RandomData])\n",
    "# #numberOfLabels = 3\n",
    "\n",
    "# #without augmentation\n",
    "# #data = np.concatenate([data_DT, data_NotDT])\n",
    "# #numberOfLabels = 2\n",
    "\n",
    "# #DT and shuffle DT\n",
    "# data = np.concatenate([data_DT, data_NotDT])\n",
    "# numberOfLabels = 2\n",
    "\n",
    "# #shuffle data\n",
    "# np.random.shuffle(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = np.concatenate([y_train_DT, y_train_NotDT, y_train_RD])\n",
    "x_data = np.concatenate([x_train_DT, x_train_NotDT, x_train_RD])\n",
    "indxs = np.arange(len(y_data))\n",
    "np.random.shuffle(indxs)\n",
    "\n",
    "y_data = y_data[indxs]\n",
    "x_data = x_data[indxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(153938, 70, 47)\n",
      "(153938, 70, 47)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_data = x_data.reshape(-1, 70, 47)\n",
    "\n",
    "N = 3\n",
    "# split train and validationy_train\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_data, y_data, test_size=0.1, random_state=2)\n",
    "y_train_onehot = to_categorical(y_train, num_classes=N)\n",
    "y_validation_onehot = to_categorical(y_validation, num_classes=N)\n",
    "\n",
    "# reshape and onehot for the 3 label validation sets\n",
    "x_validation_DT = x_validation_DT.reshape(-1, 70, 47)\n",
    "x_validation_NotDT = x_validation_NotDT.reshape(-1, 70, 47)\n",
    "x_validation_RD = x_validation_RD.reshape(-1, 70, 47)\n",
    "x_validation_RD1 = x_validation_RD1.reshape(-1, 70, 47)\n",
    "x_validation_RD2 = x_validation_RD2.reshape(-1, 70, 47)\n",
    "y_validation_onehot_DT = to_categorical(y_validation_DT, num_classes=N)\n",
    "y_validation_onehot_NotDT = to_categorical(y_validation_NotDT, num_classes=N)\n",
    "y_validation_onehot_RD = to_categorical(y_validation_RD, num_classes=N)\n",
    "\n",
    "y_validation_onehot_RD1 = to_categorical(y_validation_RD1, num_classes=2)\n",
    "y_validation_onehot_RD2 = to_categorical(y_validation_RD2, num_classes=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "(153938, 70, 47)\n",
      "(138544, 70, 47)\n",
      "(15394, 70, 47)\n",
      "(3389, 70, 47) (3389, 3)\n",
      "(5164, 70, 47) (5164, 3)\n",
      "(8553, 70, 47) (8553, 3)\n"
     ]
    }
   ],
   "source": [
    "print(maxLength)\n",
    "print(x_data.shape)\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)\n",
    "\n",
    "print(x_validation_DT.shape, y_validation_onehot_DT.shape)\n",
    "print(x_validation_NotDT.shape, y_validation_onehot_NotDT.shape)\n",
    "print(x_validation_RD.shape, y_validation_onehot_RD.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(numberOfLabels):\n",
    "    # Initialize the model to have sequential forward propagation\n",
    "    network = models.Sequential() \n",
    "    \n",
    "    # Define model's architecture    \n",
    "#     network.add(layers.Conv1D(\n",
    "#     filters=64,\n",
    "#     kernel_size= 2,\n",
    "#     padding='Same',\n",
    "#     strides=1,\n",
    "#     activation='relu',\n",
    "#     input_shape=(68,1),\n",
    "#     kernel_initializer='he_normal',\n",
    "#     bias_initializer='zeros'\n",
    "#     ))\n",
    "    \n",
    "#     network.add(layers.Conv1D(\n",
    "#     filters=64,\n",
    "#     kernel_size=3,\n",
    "#     padding='Same',\n",
    "#     strides=1,\n",
    "#     activation='relu',\n",
    "#     kernel_initializer='he_normal',\n",
    "#     bias_initializer='zeros'\n",
    "#     ))\n",
    "    \n",
    "    network.add(layers.Conv1D(\n",
    "    filters=64,\n",
    "    kernel_size=5,\n",
    "    padding='Same',\n",
    "    strides=1,\n",
    "    input_shape=(70,47),\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal',\n",
    "    bias_initializer='zeros'\n",
    "    ))\n",
    "    \n",
    "    network.add(layers.Conv1D(\n",
    "    filters=64,\n",
    "    kernel_size=7,\n",
    "    padding='Same',\n",
    "    strides=1,\n",
    "    activation='relu',\n",
    "    kernel_initializer='he_normal',\n",
    "    bias_initializer='zeros'\n",
    "    ))\n",
    "    \n",
    "#     network.add(layers.Conv1D(\n",
    "#     filters=64,\n",
    "#     kernel_size=9,\n",
    "#     padding='Same',\n",
    "#     strides=1,\n",
    "#     activation='relu',\n",
    "#     kernel_initializer='he_normal',\n",
    "#     bias_initializer='zeros'\n",
    "#     ))\n",
    "    \n",
    "    network.add(layers.Flatten())\n",
    "    \n",
    "    network.add(layers.BatchNormalization())\n",
    "    \n",
    "    network.add(layers.Dropout(0.3))\n",
    "    \n",
    "    network.add(layers.Dense(\n",
    "    N,\n",
    "    activation='softmax',\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros'\n",
    "    ))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create callback\n",
    "model_name = './sequenceData/Model/LanguageClassification-v1.0'\n",
    "checkpoint = ModelCheckpoint(model_name,\n",
    "                             monitor = 'val_loss', \n",
    "                             verbose = 1, \n",
    "                             save_best_only = True,\n",
    "                             mode = 'min',\n",
    "                             period = 1)\n",
    "\n",
    "# learning rate and decay\n",
    "learning_rate = 0.01\n",
    "decay = 1e-3\n",
    "\n",
    "# optimizer\n",
    "optimizer = optimizers.Adam()\n",
    "\n",
    "# loss\n",
    "loss = \"categorical_crossentropy\"\n",
    "\n",
    "# metrics\n",
    "metrics = [\"categorical_accuracy\"]\n",
    "\n",
    "# dropout\n",
    "dropout = 0.3\n",
    "\n",
    "# compile the model\n",
    "model = generate_model(numberOfLabels)\n",
    "model.compile(optimizer, loss, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_9 (Conv1D)            (None, 70, 64)            15104     \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 70, 64)            28736     \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 4480)              17920     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4480)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 13443     \n",
      "=================================================================\n",
      "Total params: 75,203\n",
      "Trainable params: 66,243\n",
      "Non-trainable params: 8,960\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 138544 samples, validate on 15394 samples\n",
      "Epoch 1/30\n",
      "138544/138544 [==============================] - 69s 495us/step - loss: 1.0162 - categorical_accuracy: 0.5223 - val_loss: 0.8814 - val_categorical_accuracy: 0.5741\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.88142, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 2/30\n",
      "138544/138544 [==============================] - 69s 501us/step - loss: 0.7028 - categorical_accuracy: 0.6966 - val_loss: 0.7450 - val_categorical_accuracy: 0.6453\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.88142 to 0.74504, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 3/30\n",
      "138544/138544 [==============================] - 69s 498us/step - loss: 0.5280 - categorical_accuracy: 0.7713 - val_loss: 0.6597 - val_categorical_accuracy: 0.6750\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.74504 to 0.65966, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 4/30\n",
      "138544/138544 [==============================] - 70s 505us/step - loss: 0.4307 - categorical_accuracy: 0.8107 - val_loss: 0.5468 - val_categorical_accuracy: 0.7517\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.65966 to 0.54676, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 5/30\n",
      "138544/138544 [==============================] - 67s 485us/step - loss: 0.3800 - categorical_accuracy: 0.8323 - val_loss: 0.4245 - val_categorical_accuracy: 0.8047\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.54676 to 0.42452, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 6/30\n",
      "138544/138544 [==============================] - 67s 483us/step - loss: 0.3529 - categorical_accuracy: 0.8442 - val_loss: 0.4160 - val_categorical_accuracy: 0.8100\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.42452 to 0.41604, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 7/30\n",
      "138544/138544 [==============================] - 67s 485us/step - loss: 0.3331 - categorical_accuracy: 0.8535 - val_loss: 0.3979 - val_categorical_accuracy: 0.8217\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.41604 to 0.39793, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 8/30\n",
      "138544/138544 [==============================] - 67s 483us/step - loss: 0.3182 - categorical_accuracy: 0.8609 - val_loss: 0.3491 - val_categorical_accuracy: 0.8406\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.39793 to 0.34913, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 9/30\n",
      "138544/138544 [==============================] - 67s 483us/step - loss: 0.3075 - categorical_accuracy: 0.8658 - val_loss: 0.4029 - val_categorical_accuracy: 0.8266\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.34913\n",
      "Epoch 10/30\n",
      "138544/138544 [==============================] - 67s 486us/step - loss: 0.2990 - categorical_accuracy: 0.8697 - val_loss: 0.3843 - val_categorical_accuracy: 0.8301\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.34913\n",
      "Epoch 11/30\n",
      "138544/138544 [==============================] - 67s 484us/step - loss: 0.2958 - categorical_accuracy: 0.8713 - val_loss: 0.3851 - val_categorical_accuracy: 0.8285\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.34913\n",
      "Epoch 12/30\n",
      "138544/138544 [==============================] - 67s 483us/step - loss: 0.2862 - categorical_accuracy: 0.8760 - val_loss: 0.3131 - val_categorical_accuracy: 0.8605\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.34913 to 0.31305, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 13/30\n",
      "138544/138544 [==============================] - 67s 481us/step - loss: 0.2800 - categorical_accuracy: 0.8787 - val_loss: 0.3038 - val_categorical_accuracy: 0.8653\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.31305 to 0.30375, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 14/30\n",
      "138544/138544 [==============================] - 66s 477us/step - loss: 0.2722 - categorical_accuracy: 0.8840 - val_loss: 0.2975 - val_categorical_accuracy: 0.8689\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.30375 to 0.29751, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 15/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2676 - categorical_accuracy: 0.8857 - val_loss: 0.2848 - val_categorical_accuracy: 0.8731\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.29751 to 0.28477, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 16/30\n",
      "138544/138544 [==============================] - 66s 479us/step - loss: 0.2640 - categorical_accuracy: 0.8876 - val_loss: 0.2884 - val_categorical_accuracy: 0.8740\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.28477\n",
      "Epoch 17/30\n",
      "138544/138544 [==============================] - 66s 479us/step - loss: 0.2605 - categorical_accuracy: 0.8882 - val_loss: 0.3217 - val_categorical_accuracy: 0.8580\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.28477\n",
      "Epoch 18/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2560 - categorical_accuracy: 0.8910 - val_loss: 0.3196 - val_categorical_accuracy: 0.8606\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.28477\n",
      "Epoch 19/30\n",
      "138544/138544 [==============================] - 66s 477us/step - loss: 0.2520 - categorical_accuracy: 0.8926 - val_loss: 0.3028 - val_categorical_accuracy: 0.8662\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.28477\n",
      "Epoch 20/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2482 - categorical_accuracy: 0.8940 - val_loss: 0.2759 - val_categorical_accuracy: 0.8784\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.28477 to 0.27588, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 21/30\n",
      "138544/138544 [==============================] - 66s 479us/step - loss: 0.2472 - categorical_accuracy: 0.8940 - val_loss: 0.2772 - val_categorical_accuracy: 0.8800\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27588\n",
      "Epoch 22/30\n",
      "138544/138544 [==============================] - 67s 480us/step - loss: 0.2433 - categorical_accuracy: 0.8965 - val_loss: 0.2796 - val_categorical_accuracy: 0.8774\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27588\n",
      "Epoch 23/30\n",
      "138544/138544 [==============================] - 67s 480us/step - loss: 0.2393 - categorical_accuracy: 0.8989 - val_loss: 0.2841 - val_categorical_accuracy: 0.8766\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27588\n",
      "Epoch 24/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2374 - categorical_accuracy: 0.8996 - val_loss: 0.2742 - val_categorical_accuracy: 0.8811\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.27588 to 0.27424, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 25/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2329 - categorical_accuracy: 0.9012 - val_loss: 0.2841 - val_categorical_accuracy: 0.8779\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27424\n",
      "Epoch 26/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2315 - categorical_accuracy: 0.9021 - val_loss: 0.2909 - val_categorical_accuracy: 0.8679\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27424\n",
      "Epoch 27/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2301 - categorical_accuracy: 0.9029 - val_loss: 0.2711 - val_categorical_accuracy: 0.8842\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.27424 to 0.27111, saving model to ./sequenceData/Model/LanguageClassification-v1.0\n",
      "Epoch 28/30\n",
      "138544/138544 [==============================] - 66s 479us/step - loss: 0.2259 - categorical_accuracy: 0.9048 - val_loss: 0.2856 - val_categorical_accuracy: 0.8747\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.27111\n",
      "Epoch 29/30\n",
      "138544/138544 [==============================] - 66s 477us/step - loss: 0.2233 - categorical_accuracy: 0.9055 - val_loss: 0.2759 - val_categorical_accuracy: 0.8785\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.27111\n",
      "Epoch 30/30\n",
      "138544/138544 [==============================] - 66s 478us/step - loss: 0.2210 - categorical_accuracy: 0.9076 - val_loss: 0.2718 - val_categorical_accuracy: 0.8826\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.27111\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15523950cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 30 \n",
      " Batch_size = 10000 \n",
      " learning_rate = 0.01 decay = 0.001 Dropout = 0.3 Time elapsed (min): 33.50628256400426\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "batch_size = 10000\n",
    "\n",
    "tra_losses = []\n",
    "tra_accus = []\n",
    "\n",
    "start_time = time.time()\n",
    "# train the model\n",
    "# for e in range(n_epochs):\n",
    "results = model.fit(x_train, y_train_onehot, \n",
    "                    epochs=n_epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    validation_data=(x_validation, y_validation_onehot),\n",
    "                   callbacks = [checkpoint] )\n",
    "    \n",
    "tra_losses = results.history['loss']\n",
    "tra_accus = results.history['categorical_accuracy']\n",
    "\n",
    "elapsed_t = time.time() - start_time\n",
    "plt.plot(tra_losses)\n",
    "plt.plot(tra_accus)\n",
    "plt.show()\n",
    "\n",
    "print(\"Epochs {} \\n Batch_size = {} \\n learning_rate = {} decay = {} Dropout = {} Time elapsed (min): {}\".format(\n",
    "    n_epochs, batch_size, learning_rate, decay, dropout, elapsed_t/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores_full_data\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6c824605e37a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"scores_full_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_validation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_validation_onehot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"scores_full_data\")\n",
    "scores = model.evaluate(x_validation, y_validation_onehot) \n",
    "print(\"loss\", scores[0])\n",
    "print(\"accuracy\", scores[1])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"scores_DT\")\n",
    "scores_DT = model.evaluate(x_validation_DT, y_validation_onehot_DT) \n",
    "print(\"loss\", scores_DT[0])\n",
    "print(\"accuracy\", scores_DT[1])\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"scores_NotDT\")\n",
    "scores_NotDT = model.evaluate(x_validation_NotDT, y_validation_onehot_NotDT) \n",
    "print(\"loss\", scores_NotDT[0])\n",
    "print(\"accuracy\", scores_NotDT[1])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"scores_RD\")\n",
    "scores_RD = model.evaluate(x_validation_RD, y_validation_onehot_RD) \n",
    "print(\"loss\", scores_RD[0])\n",
    "print(\"accuracy\", scores_RD[1])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"scores_RD1\")\n",
    "scores_RD1 = model.evaluate(x_validation_RD1, y_validation_onehot_RD1) \n",
    "print(\"loss\", scores_RD1[0])\n",
    "print(\"accuracy\", scores_RD1[1])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"scores_RD2\")\n",
    "scores_RD2 = model.evaluate(x_validation_RD2, y_validation_onehot_RD2) \n",
    "print(\"loss\", scores_RD2[0])\n",
    "print(\"accuracy\", scores_RD2[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

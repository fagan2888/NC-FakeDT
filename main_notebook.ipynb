{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pablo/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import keras\n",
    "from keras import Model\n",
    "\n",
    "# out libraries\n",
    "import Utils.W2V as wor2vec\n",
    "from Utils.classifier import create_Trumpifier\n",
    "from Utils.genetic_algorithm import GeneticAlgorithm\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# random seed\n",
    "rseed = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 11:58:58,604 : INFO : loading Word2Vec object from ./Utils/w2v_models/Final_w2v_only_tweets\n",
      "2018-06-20 11:58:59,340 : INFO : loading wv recursively from ./Utils/w2v_models/Final_w2v_only_tweets.wv.* with mmap=None\n",
      "2018-06-20 11:58:59,341 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-20 11:58:59,342 : INFO : loading vocabulary recursively from ./Utils/w2v_models/Final_w2v_only_tweets.vocabulary.* with mmap=None\n",
      "2018-06-20 11:58:59,344 : INFO : loading trainables recursively from ./Utils/w2v_models/Final_w2v_only_tweets.trainables.* with mmap=None\n",
      "2018-06-20 11:58:59,345 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-20 11:58:59,348 : INFO : loaded ./Utils/w2v_models/Final_w2v_only_tweets\n",
      "2018-06-20 11:58:59,594 : INFO : saving Word2Vec object under ./Utils/w2v_models/Final_w2v_only_tweets, separately None\n",
      "2018-06-20 11:58:59,595 : INFO : not storing attribute vectors_norm\n",
      "2018-06-20 11:58:59,596 : INFO : not storing attribute cum_table\n",
      "2018-06-20 11:59:00,630 : INFO : saved ./Utils/w2v_models/Final_w2v_only_tweets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length input = 171044\n",
      "Length output = 171044\n",
      "Unknown words = 0\n"
     ]
    }
   ],
   "source": [
    "# load raw data\n",
    "notTrump_df = pd.read_csv('processedData/processedDataNotDonaldTrump.csv',delimiter=\"|\", header=None)\n",
    "Trump_df = pd.read_csv('processedData/processedDataDonaldTrump.csv',delimiter=\"|\", header=None)\n",
    "\n",
    "# delete special characters, generate labels and training data for word2vec\n",
    "X_t = [\"\".join(re.split('[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]', t)).split() for t in Trump_df[1].values.tolist()]\n",
    "y_t = np.zeros(len(X_t), dtype=int)\n",
    "X_nt = [\"\".join(re.split('[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]', t)).split() for t in notTrump_df[1].values.tolist()]\n",
    "y_nt = np.ones(len(X_nt), dtype=int)\n",
    "\n",
    "# make random combinations of each tweet\n",
    "X_random_t = []\n",
    "X_random_nt = []\n",
    "\n",
    "for i in range(len(X_t)):\n",
    "    row = X_t[i].copy()\n",
    "    np.random.shuffle(row)\n",
    "    X_random_t.append(row)\n",
    "\n",
    "for i in range(len(X_nt)):\n",
    "    row = X_nt[i].copy()\n",
    "    np.random.shuffle(row)\n",
    "    X_random_nt.append(row)\n",
    "    \n",
    "y_random_t = np.ones(len(X_random_t), dtype=int) * 2\n",
    "y_random_nt = np.ones(len(X_random_nt), dtype=int) * 2\n",
    "\n",
    "\n",
    "# Convert all data to vectors\n",
    "vec_size = 100\n",
    "tweet_len = 70\n",
    "\n",
    "w2v_ds = np.concatenate((X_t, X_nt, X_random_t, X_random_nt))\n",
    "\n",
    "USE_NO_TWEET_W = False\n",
    "\n",
    "w2v = wor2vec.W2V(out_size = vec_size, fname=\"Final_w2v_only_tweets\")\n",
    "# w2v.create_w2v(w2v_ds, USE_NO_TWEET_W)\n",
    "w2v.load_w2v()\n",
    "w2v_dict = w2v.dictionary\n",
    "\n",
    "if USE_NO_TWEET_W:\n",
    "    # create random data including words that are not inthe tweets\n",
    "    new_random_X = np.random.choice(list(w2v_dict.keys()), (len(X_t), tweet_len))\n",
    "    offsets = np.random.randint(1, tweet_len, len(new_random_X))\n",
    "    new_random_X = [new_random_X[i][:offsets[i]] for i in range(len(new_random_X))]\n",
    "    new_random_X = [list(c) for c in new_random_X]\n",
    "    new_random_y = np.ones(len(new_random_X), dtype=int) * 2\n",
    "\n",
    "    # merge all data\n",
    "    X = np.concatenate((X_t, X_nt, X_random_t, X_random_nt, new_random_X))\n",
    "    y = np.concatenate((y_t, y_nt, y_random_t, y_random_nt, new_random_y))\n",
    "    y = keras.utils.to_categorical(y)\n",
    "    \n",
    "else:\n",
    "    # merge all data\n",
    "    X = np.concatenate((X_t, X_nt, X_random_t, X_random_nt))\n",
    "    y = np.concatenate((y_t, y_nt, y_random_t, y_random_nt))\n",
    "    y = keras.utils.to_categorical(y)\n",
    "    \n",
    "\n",
    "X_vec = w2v.vectorize_words(X, tweet_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split train and test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.20, shuffle=True, random_state=rseed)\n",
    "\n",
    "# X_train = np.array([np.array(x) for x in X_train])\n",
    "# X_test = np.array([np.array(x) for x in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# classifier = create_Trumpifier(in_size = (tweet_len, vec_size), out_size = len(y[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trumpifier_name = \"Trumpifier_onlytweets.h5\"\n",
    "# Trumpifier_name = \"First_trumpifier.h5\"\n",
    "Trumpifier_name = \"Final_only_tweets_trumpifier.h5\"\n",
    "\n",
    "classifier = keras.models.load_model(os.path.join(\"./Utils/w2v_models/\", Trumpifier_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classifier.evaluate(X_test, y_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Fit the model\n",
    "# checkpoint = ModelCheckpoint(os.path.join(\"./Utils/w2v_models/\", Trumpifier_name), \n",
    "#                              monitor='val_loss', \n",
    "#                              verbose=1, \n",
    "#                              save_best_only=True)\n",
    "\n",
    "# classifier.fit(X_train, y_train, \n",
    "#                validation_split = 0.2,\n",
    "#                epochs = 100, \n",
    "#                batch_size = 1024,\n",
    "#                callbacks = [checkpoint]\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier.evaluate(X_test, y_test, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-20 11:55:45,132 : INFO : loading Word2Vec object from ./Utils/w2v_models/Final_w2v_only_tweets\n",
      "2018-06-20 11:55:46,477 : INFO : loading wv recursively from ./Utils/w2v_models/Final_w2v_only_tweets.wv.* with mmap=None\n",
      "2018-06-20 11:55:46,478 : INFO : setting ignored attribute vectors_norm to None\n",
      "2018-06-20 11:55:46,479 : INFO : loading vocabulary recursively from ./Utils/w2v_models/Final_w2v_only_tweets.vocabulary.* with mmap=None\n",
      "2018-06-20 11:55:46,480 : INFO : loading trainables recursively from ./Utils/w2v_models/Final_w2v_only_tweets.trainables.* with mmap=None\n",
      "2018-06-20 11:55:46,480 : INFO : setting ignored attribute cum_table to None\n",
      "2018-06-20 11:55:46,482 : INFO : loaded ./Utils/w2v_models/Final_w2v_only_tweets\n",
      "2018-06-20 11:55:46,738 : INFO : saving Word2Vec object under ./Utils/w2v_models/Final_w2v_only_tweets, separately None\n",
      "2018-06-20 11:55:46,739 : INFO : not storing attribute vectors_norm\n",
      "2018-06-20 11:55:46,740 : INFO : not storing attribute cum_table\n",
      "2018-06-20 11:55:48,079 : INFO : saved ./Utils/w2v_models/Final_w2v_only_tweets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetic algorithm with:\n",
      "\t- population size: 100\n",
      "\t- crossover rate: 0.1\n",
      "\t- mutation rate: 0.01\n",
      "\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  Searched in:\n    - '/home/pablo/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/pablo/anaconda3/envs/MLiP/nltk_data'\n    - '/home/pablo/anaconda3/envs/MLiP/share/nltk_data'\n    - '/home/pablo/anaconda3/envs/MLiP/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b97ce53d9633>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                       evaluationPercentage = [0.2, 0.2, 0.2, 0.2, 0.2])\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfinal_tweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fit_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_fit_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_len_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_len_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_algorithm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Natural Computing/natural-computing/GAforDonaldTrumpTweets/Utils/genetic_algorithm.py\u001b[0m in \u001b[0;36mstart_algorithm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# evaluate it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mfitness\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_population\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#create elapsed arrays for time measuring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Natural Computing/natural-computing/GAforDonaldTrumpTweets/Utils/genetic_algorithm.py\u001b[0m in \u001b[0;36mevaluate_population\u001b[0;34m(self, pop)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m#get the language score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mlanguage_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_language\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0mlanguage_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluationPercentage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Natural Computing/natural-computing/GAforDonaldTrumpTweets/Utils/LanguageModel.py\u001b[0m in \u001b[0;36mpredict_language\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0;31m#get the tags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mtagged\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 \u001b[0;31m#convet the tags to numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mtagsN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLiP/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLiP/lib/python3.6/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLiP/lib/python3.6/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/MLiP/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  Searched in:\n    - '/home/pablo/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/home/pablo/anaconda3/envs/MLiP/nltk_data'\n    - '/home/pablo/anaconda3/envs/MLiP/share/nltk_data'\n    - '/home/pablo/anaconda3/envs/MLiP/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = [\"fake\", \"news\"]\n",
    "\n",
    "ite = 50\n",
    "GA = GeneticAlgorithm(topics = t, pop_size = 100, n_iter = ite, fitness_model = classifier, \n",
    "                      vocab = w2v_dict, cross_rate = 0.1, mut_rate = 0.01, \n",
    "                      ind_shape = (tweet_len, vec_size), \n",
    "                      evaluationPercentage = [0.2, 0.2, 0.2, 0.2, 0.2])\n",
    "\n",
    "final_tweet, max_fit_hist, avg_fit_hist, max_len_hist, min_len_hist, avg_len_hist = GA.start_algorithm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_tweet)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "line1, = plt.plot(max_fit_hist, label='Max fitness')\n",
    "line2, = plt.plot(avg_fit_hist, label='Average fitness')\n",
    "plt.legend(handles=[line1, line2])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "line1, = plt.plot(max_len_hist, label='Max length')\n",
    "line2, = plt.plot(min_len_hist, label='Min length')\n",
    "line3, = plt.plot(avg_len_hist, label='Average length')\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "directoryDT = './processedData/TestDonaldTrump_tweets.csv'\n",
    "\n",
    "DTdata_pd = pd.read_csv(directoryDT, delimiter='|')\n",
    "\n",
    "a = np.array(DTdata_pd.values.tolist())\n",
    "b = list(a[:,1])\n",
    "\n",
    "# delete special characters, generate labels and training data for word2vec\n",
    "X_t = [\"\".join(re.split('[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]', t)).split() for t in b]\n",
    "\n",
    "population_words = X_t[10:11]+ X_t[1:2]\n",
    "pop_size = len(population_words)\n",
    "ind_shape = (70, 100)\n",
    "vocab = w2v_dict\n",
    "\n",
    "population_init = [[( iw , vocab[iw][0]) for iw in ind_words] for ind_words in population_words]\n",
    "\n",
    "\n",
    "for i in range(len(population_init)):                \n",
    "    #add zero tuples at the end\n",
    "    cut_offset = len(population_init[i])\n",
    "    for j in range(ind_shape[0] - cut_offset):\n",
    "        population_init[i].append(  ('' , np.zeros(ind_shape[1], dtype = np.float32) ) )\n",
    "popu = population_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for i in popu:\n",
    "    tweets.append(' '.join( np.array([x for x,_ in i])) )\n",
    "    \n",
    "print(tweets)\n",
    "a = GA.evaluate_population(popu)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
